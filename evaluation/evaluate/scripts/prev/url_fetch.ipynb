{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "import re\n",
    "import string\n",
    "from typing import Optional, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 自定义请求头\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "#     'Referer': 'https://www.google.com/'  # Referer有时也会影响访问权限\n",
    "# }\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "jina_headers = {\n",
    "    'Authorization': 'Bearer jina_c3839dcd54ad44b29a2922aec781cd88b7bjgVl0avG0q4yJ2MUWblSXEVlP',\n",
    "    'X-Return-Format': 'markdown',\n",
    "    # 'X-With-Links-Summary': 'true'\n",
    "}\n",
    "\n",
    "# 初始化会话\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "# 从PDF中提取文本\n",
    "def extract_pdf_text(url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=20)  # 设置超时时间为20秒\n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: Unable to retrieve the PDF (status code {response.status_code})\"\n",
    "        \n",
    "        # 使用pdfplumber打开PDF文件\n",
    "        with pdfplumber.open(BytesIO(response.content)) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    full_text += text\n",
    "        \n",
    "        # 限制文本长度\n",
    "        cleaned_text = ' '.join(full_text.split()[:1000])\n",
    "        return cleaned_text\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out after 10 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_url(url, use_jina=False, snippet: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    从 URL 中提取文本。如果提供了 snippet，则提取与之相关的上下文。\n",
    "\n",
    "    Args:\n",
    "        url (str): 网页或 PDF 的 URL。\n",
    "        use_jina (bool): 是否使用 Jina 进行提取。\n",
    "        snippet (Optional[str]): 要查找的片段。\n",
    "\n",
    "    Returns:\n",
    "        str: 提取的文本或上下文。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if use_jina:\n",
    "            response = requests.get(f'https://r.jina.ai/{url}', headers=jina_headers).text\n",
    "            # 去除 URL\n",
    "            pattern = r\"\\(https?:.*?\\)|\\[https?:.*?\\]\"\n",
    "            text = re.sub(pattern, \"\", response).replace('---','-').replace('===','=').replace('   ',' ').replace('   ',' ')\n",
    "        else:\n",
    "            response = session.get(url, timeout=20)  # 设置超时时间为20秒\n",
    "            response.raise_for_status()  # 如果请求失败，抛出 HTTPError\n",
    "            # 判断返回的内容类型\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'pdf' in content_type:\n",
    "                # 如果是 PDF 文件，提取 PDF 文本\n",
    "                return extract_pdf_text(url)\n",
    "            # 尝试使用 lxml 解析，如果不可用则使用 html.parser\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "            except Exception:\n",
    "                print(\"lxml parser not found or failed, falling back to html.parser\")\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        return text\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"HTTP error occurred: {http_err}\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return \"Error: Connection error occurred\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out after 20 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "def fetch_page_content(urls, max_workers=32, use_jina=False, snippets: Optional[dict] = None):\n",
    "    \"\"\"\n",
    "    并发地从多个 URL 中获取内容。\n",
    "\n",
    "    Args:\n",
    "        urls (list): 要抓取的 URL 列表。\n",
    "        max_workers (int): 最大并发线程数。\n",
    "        use_jina (bool): 是否使用 Jina 进行提取。\n",
    "        snippets (Optional[dict]): 一个字典，将 URL 映射到相应的片段。\n",
    "\n",
    "    Returns:\n",
    "        dict: 一个字典，将 URL 映射到提取的内容或上下文。\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 使用 tqdm 显示进度条\n",
    "        futures = {\n",
    "            executor.submit(extract_text_from_url, url, use_jina, snippets.get(url) if snippets else None): url\n",
    "            for url in urls\n",
    "        }\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), desc=\"Fetching URLs\", total=len(urls)):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                results[url] = data\n",
    "            except Exception as exc:\n",
    "                results[url] = f\"Error fetching {url}: {exc}\"\n",
    "            # time.sleep(0.1)  # 简单的速率限制\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs to fetch: 47125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching URLs:   1%|          | 239/47125 [01:32<9:29:30,  1.37it/s] /tmp/ipykernel_22384/1389475620.py:94: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.text, 'lxml')\n",
      "Fetching URLs:  33%|███▎      | 15734/47125 [2:52:35<10:12:05,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 定义输入和输出路径\n",
    "    url_cache_path = \"/fs/archive/share/u2023000153/Search-o1/cache/url_cache.json\"\n",
    "    url_cache_new1_path = \"/fs/archive/share/u2023000153/Search-o1/cache/url_cache_new1.json\"\n",
    "    \n",
    "    # 读取url_cache文件\n",
    "    with open(url_cache_path, 'r', encoding='utf-8') as f:\n",
    "        url_cache = json.load(f)\n",
    "    \n",
    "    # 提取所有错误的URL\n",
    "    urls = []\n",
    "    error_indicators = [\n",
    "        'limit exceeded',\n",
    "        'Error fetching URL',\n",
    "        'Account balance not enough to run this query, please recharge.',\n",
    "        'Invalid bearer token',\n",
    "        'HTTP error occurred', \n",
    "        'Error: Connection error occurred',\n",
    "        'Error: Request timed out',\n",
    "        'Unexpected error',\n",
    "        'Please turn on Javascript'\n",
    "    ]\n",
    "    \n",
    "    for url, page in url_cache.items():\n",
    "        if isinstance(page, str) and (any(indicator.lower() in page.lower() for indicator in error_indicators) or page.startswith('http') or len(page) < 10):\n",
    "            urls.append(url)\n",
    "            \n",
    "    print(f\"Total URLs to fetch: {len(urls)}\")\n",
    "    \n",
    "    # 重新获取错误URL的内容\n",
    "    if urls:\n",
    "        new_cache = fetch_page_content(urls, use_jina=False)\n",
    "        \n",
    "        # 将新获取的内容合并到原始缓存中\n",
    "        for url, content in new_cache.items():\n",
    "            url_cache[url] = content\n",
    "        \n",
    "    # 保存完整的url_cache到新文件\n",
    "    with open(url_cache_new1_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(url_cache, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Updated complete cache saved to {url_cache_new1_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original cache size: 170617\n",
      "New cache 1 size: 10311\n",
      "Merged cache size: 170617\n",
      "Merged cache saved to /fs/archive/share/u2023000153/Search-o1/cache/url_cache_new2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the two cache files\n",
    "cache_path = '/fs/archive/share/u2023000153/Search-o1/cache/url_cache.json'\n",
    "cache_path_new1 = '/fs/archive/share/u2023000153/Search-o1/cache/url_cache_new1.json'\n",
    "output_path_new2 = '/fs/archive/share/u2023000153/Search-o1/cache/url_cache_new2.json'\n",
    "\n",
    "with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "    url_cache = json.load(f)\n",
    "    \n",
    "with open(cache_path_new1, 'r', encoding='utf-8') as f:\n",
    "    url_cache_new1 = json.load(f)\n",
    "\n",
    "# Define error indicators\n",
    "error_indicators = [\n",
    "    'limit exceeded',\n",
    "    'Error fetching URL',\n",
    "    'Account balance not enough to run this query, please recharge.',\n",
    "    'Invalid bearer token', \n",
    "    'HTTP error occurred',\n",
    "    'Error: Connection error occurred',\n",
    "    'Error: Request timed out',\n",
    "    'Unexpected error',\n",
    "    'Please turn on Javascript'\n",
    "]\n",
    "\n",
    "# Merge caches, preferring valid content\n",
    "merged_cache = {}\n",
    "for url in set(url_cache.keys()) | set(url_cache_new1.keys()):\n",
    "    content1 = url_cache.get(url, '')\n",
    "    content2 = url_cache_new1.get(url, '')\n",
    "    \n",
    "    # Check if contents are error messages\n",
    "    is_error1 = isinstance(content1, str) and (any(indicator in content1 for indicator in error_indicators) or content1.startswith('http'))\n",
    "    is_error2 = isinstance(content2, str) and (any(indicator in content2 for indicator in error_indicators) or content2.startswith('http'))\n",
    "    \n",
    "    # Take valid content if available\n",
    "    if not is_error1:\n",
    "        merged_cache[url] = content1\n",
    "    elif not is_error2:\n",
    "        merged_cache[url] = content2\n",
    "    else:\n",
    "        # If both are errors, take the first one\n",
    "        merged_cache[url] = content1\n",
    "\n",
    "# Save merged cache\n",
    "with open(output_path_new2, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_cache, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Original cache size: {len(url_cache)}\")\n",
    "print(f\"New cache 1 size: {len(url_cache_new1)}\")\n",
    "print(f\"Merged cache size: {len(merged_cache)}\")\n",
    "print(f\"Merged cache saved to {output_path_new2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "import re\n",
    "import string\n",
    "from typing import Optional, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict, Union\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "# ----------------------- Custom Headers -----------------------\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/58.0.3029.110 Safari/537.36',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Initialize session\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "\n",
    "class WebParserClient:\n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        \"\"\"\n",
    "        初始化Web解析器客户端\n",
    "        \n",
    "        Args:\n",
    "            base_url: API服务器的基础URL，默认为本地测试服务器\n",
    "        \"\"\"\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        \n",
    "    def parse_urls(self, urls: List[str], timeout: int = 200) -> List[Dict[str, Union[str, bool]]]:\n",
    "        \"\"\"\n",
    "        发送URL列表到解析服务器并获取解析结果\n",
    "        \n",
    "        Args:\n",
    "            urls: 需要解析的URL列表\n",
    "            timeout: 请求超时时间，默认20秒\n",
    "            \n",
    "        Returns:\n",
    "            解析结果列表\n",
    "            \n",
    "        Raises:\n",
    "            requests.exceptions.RequestException: 当API请求失败时\n",
    "            requests.exceptions.Timeout: 当请求超时时\n",
    "        \"\"\"\n",
    "        endpoint = urljoin(self.base_url, \"/parse_urls\")\n",
    "        response = requests.post(endpoint, json={\"urls\": urls}, timeout=timeout)\n",
    "        response.raise_for_status()  # 如果响应状态码不是200，抛出异常\n",
    "        \n",
    "        return response.json()[\"results\"]\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"Remove punctuation from the text.\"\"\"\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def f1_score(true_set: set, pred_set: set) -> float:\n",
    "    \"\"\"Calculate the F1 score between two sets of words.\"\"\"\n",
    "    intersection = len(true_set.intersection(pred_set))\n",
    "    if not intersection:\n",
    "        return 0.0\n",
    "    precision = intersection / float(len(pred_set))\n",
    "    recall = intersection / float(len(true_set))\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def extract_snippet_with_context(full_text: str, snippet: str, context_chars: int = 2500) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Extract the sentence that best matches the snippet and its context from the full text.\n",
    "\n",
    "    Args:\n",
    "        full_text (str): The full text extracted from the webpage.\n",
    "        snippet (str): The snippet to match.\n",
    "        context_chars (int): Number of characters to include before and after the snippet.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, str]: The first element indicates whether extraction was successful, the second element is the extracted context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_text = full_text[:50000]\n",
    "\n",
    "        snippet = snippet.lower()\n",
    "        snippet = remove_punctuation(snippet)\n",
    "        snippet_words = set(snippet.split())\n",
    "\n",
    "        best_sentence = None\n",
    "        best_f1 = 0.2\n",
    "\n",
    "        # sentences = re.split(r'(?<=[.!?]) +', full_text)  # Split sentences using regex, supporting ., !, ? endings\n",
    "        sentences = sent_tokenize(full_text)  # Split sentences using nltk's sent_tokenize\n",
    "\n",
    "        for sentence in sentences:\n",
    "            key_sentence = sentence.lower()\n",
    "            key_sentence = remove_punctuation(key_sentence)\n",
    "            sentence_words = set(key_sentence.split())\n",
    "            f1 = f1_score(snippet_words, sentence_words)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_sentence = sentence\n",
    "\n",
    "        if best_sentence:\n",
    "            para_start = full_text.find(best_sentence)\n",
    "            para_end = para_start + len(best_sentence)\n",
    "            start_index = max(0, para_start - context_chars)\n",
    "            end_index = min(len(full_text), para_end + context_chars)\n",
    "            context = full_text[start_index:end_index]\n",
    "            return True, context\n",
    "        else:\n",
    "            # If no matching sentence is found, return the first context_chars*2 characters of the full text\n",
    "            return False, full_text[:context_chars * 2]\n",
    "    except Exception as e:\n",
    "        return False, f\"Failed to extract snippet context due to {str(e)}\"\n",
    "\n",
    "def extract_text_from_url(url, use_jina=False, jina_api_key=None, snippet: Optional[str] = None, keep_links=False):\n",
    "    \"\"\"\n",
    "    Extract text from a URL. If a snippet is provided, extract the context related to it.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of a webpage or PDF.\n",
    "        use_jina (bool): Whether to use Jina for extraction.\n",
    "        jina_api_key (str): API key for Jina.\n",
    "        snippet (Optional[str]): The snippet to search for.\n",
    "        keep_links (bool): Whether to keep links in the extracted text.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text or context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if use_jina:\n",
    "            jina_headers = {\n",
    "                'Authorization': f'Bearer {jina_api_key}',\n",
    "                'X-Return-Format': 'markdown',\n",
    "            }\n",
    "            response = requests.get(f'https://r.jina.ai/{url}', headers=jina_headers).text\n",
    "            # Remove URLs\n",
    "            pattern = r\"\\(https?:.*?\\)|\\[https?:.*?\\]\"\n",
    "            text = re.sub(pattern, \"\", response).replace('---','-').replace('===','=').replace('   ',' ').replace('   ',' ')\n",
    "        else:\n",
    "            if 'pdf' in url:\n",
    "                # If it's a PDF file, extract PDF text\n",
    "                return extract_pdf_text(url)\n",
    "\n",
    "            try:\n",
    "                response = session.get(url, timeout=30)  # Set timeout to 20 seconds\n",
    "                response.raise_for_status()  # Raise HTTPError if the request failed\n",
    "                # Determine the content type\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                \n",
    "                # Try using lxml parser, fallback to html.parser if unavailable\n",
    "                try:\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                except Exception:\n",
    "                    print(\"lxml parser not found or failed, falling back to html.parser\")\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Check if content has error indicators\n",
    "                error_indicators = [\n",
    "                    'limit exceeded',\n",
    "                    'Error fetching URL',\n",
    "                    'Account balance not enough to run this query, please recharge.',\n",
    "                    'Invalid bearer token',\n",
    "                    'HTTP error occurred', \n",
    "                    'Error: Connection error occurred',\n",
    "                    'Error: Request timed out',\n",
    "                    'Unexpected error',\n",
    "                    'Please turn on Javascript'\n",
    "                ]\n",
    "\n",
    "                has_error = any(indicator.lower() in response.text.lower() for indicator in error_indicators)\n",
    "                if has_error:\n",
    "                    # If content has error, use WebParserClient as fallback\n",
    "                    client = WebParserClient(\"http://183.174.229.164:1241\")\n",
    "                    results = client.parse_urls([url])\n",
    "                    if results and results[0][\"success\"]:\n",
    "                        text = results[0][\"content\"]\n",
    "                    else:\n",
    "                        error_msg = results[0].get(\"error\", \"Unknown error\") if results else \"No results returned\"\n",
    "                        return f\"WebParserClient error: {error_msg}\"\n",
    "                else:\n",
    "                    if keep_links:\n",
    "                        # Clean and extract main content\n",
    "                        # Remove script, style tags etc\n",
    "                        for element in soup.find_all(['script', 'style', 'meta', 'link']):\n",
    "                            element.decompose()\n",
    "\n",
    "                        # Extract text and links\n",
    "                        text_parts = []\n",
    "                        for element in soup.body.descendants if soup.body else soup.descendants:\n",
    "                            if isinstance(element, str) and element.strip():\n",
    "                                # Clean extra whitespace\n",
    "                                cleaned_text = ' '.join(element.strip().split())\n",
    "                                if cleaned_text:\n",
    "                                    text_parts.append(cleaned_text)\n",
    "                            elif element.name == 'a' and element.get('href'):\n",
    "                                href = element.get('href')\n",
    "                                link_text = element.get_text(strip=True)\n",
    "                                if href and link_text:  # Only process a tags with both text and href\n",
    "                                    # Handle relative URLs\n",
    "                                    if href.startswith('/'):\n",
    "                                        base_url = '/'.join(url.split('/')[:3])\n",
    "                                        href = base_url + href\n",
    "                                    elif not href.startswith(('http://', 'https://')):\n",
    "                                        href = url.rstrip('/') + '/' + href\n",
    "                                    text_parts.append(f\"[{link_text}]({href})\")\n",
    "\n",
    "                        # Merge text with reasonable spacing\n",
    "                        text = ' '.join(text_parts)\n",
    "                        # Clean extra spaces\n",
    "                        text = ' '.join(text.split())\n",
    "                    else:\n",
    "                        text = soup.get_text(separator=' ', strip=True)\n",
    "            except Exception as e:\n",
    "                # If normal extraction fails, try using WebParserClient\n",
    "                client = WebParserClient(\"http://183.174.229.164:1241\")\n",
    "                results = client.parse_urls([url])\n",
    "                if results and results[0][\"success\"]:\n",
    "                    text = results[0][\"content\"]\n",
    "                else:\n",
    "                    error_msg = results[0].get(\"error\", \"Unknown error\") if results else \"No results returned\"\n",
    "                    return f\"WebParserClient error: {error_msg}\"\n",
    "\n",
    "        if snippet:\n",
    "            success, context = extract_snippet_with_context(text, snippet)\n",
    "            if success:\n",
    "                return context\n",
    "            else:\n",
    "                return text\n",
    "        else:\n",
    "            # If no snippet is provided, return directly\n",
    "            return text[:30000]\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"HTTP error occurred: {http_err}\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return \"Error: Connection error occurred\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out after 20 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "def fetch_page_content(urls, max_workers=32, use_jina=False, jina_api_key=None, snippets: Optional[dict] = None, show_progress=False, keep_links=False):\n",
    "    \"\"\"\n",
    "    Concurrently fetch content from multiple URLs.\n",
    "\n",
    "    Args:\n",
    "        urls (list): List of URLs to scrape.\n",
    "        max_workers (int): Maximum number of concurrent threads.\n",
    "        use_jina (bool): Whether to use Jina for extraction.\n",
    "        jina_api_key (str): API key for Jina.\n",
    "        snippets (Optional[dict]): A dictionary mapping URLs to their respective snippets.\n",
    "        show_progress (bool): Whether to show progress bar with tqdm.\n",
    "        keep_links (bool): Whether to keep links in the extracted text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping URLs to the extracted content or context.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(extract_text_from_url, url, use_jina, jina_api_key, snippets.get(url) if snippets else None, keep_links): url\n",
    "            for url in urls\n",
    "        }\n",
    "        completed_futures = concurrent.futures.as_completed(futures)\n",
    "        if show_progress:\n",
    "            completed_futures = tqdm(completed_futures, desc=\"Fetching URLs\", total=len(urls))\n",
    "            \n",
    "        for future in completed_futures:\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                results[url] = data\n",
    "            except Exception as exc:\n",
    "                results[url] = f\"Error fetching {url}: {exc}\"\n",
    "            # time.sleep(0.1)  # Simple rate limiting\n",
    "    return results\n",
    "\n",
    "def bing_web_search(query, subscription_key, endpoint, market='en-US', language='en', timeout=20):\n",
    "    \"\"\"\n",
    "    Perform a search using the Bing Web Search API with a set timeout.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query.\n",
    "        subscription_key (str): Subscription key for the Bing Search API.\n",
    "        endpoint (str): Endpoint for the Bing Search API.\n",
    "        market (str): Market, e.g., \"en-US\" or \"zh-CN\".\n",
    "        language (str): Language of the results, e.g., \"en\".\n",
    "        timeout (int or float or tuple): Request timeout in seconds.\n",
    "                                         Can be a float representing the total timeout,\n",
    "                                         or a tuple (connect timeout, read timeout).\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response of the search results. Returns empty dict if all retries fail.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": subscription_key\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"mkt\": market,\n",
    "        \"setLang\": language,\n",
    "        \"textDecorations\": True,\n",
    "        \"textFormat\": \"HTML\"\n",
    "    }\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(endpoint, headers=headers, params=params, timeout=timeout)\n",
    "            response.raise_for_status()  # Raise exception if the request failed\n",
    "            search_results = response.json()\n",
    "            return search_results\n",
    "        except Timeout:\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(f\"Bing Web Search request timed out ({timeout} seconds) for query: {query} after {max_retries} retries\")\n",
    "                return {}\n",
    "            print(f\"Bing Web Search Timeout occurred, retrying ({retry_count}/{max_retries})...\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(f\"Bing Web Search Request Error occurred: {e} after {max_retries} retries\")\n",
    "                return {}\n",
    "            print(f\"Bing Web Search Request Error occurred, retrying ({retry_count}/{max_retries})...\")\n",
    "        time.sleep(1)  # Wait 1 second between retries\n",
    "    \n",
    "    return {}  # Should never reach here but added for completeness\n",
    "\n",
    "\n",
    "def extract_pdf_text(url):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=20)  # Set timeout to 20 seconds\n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: Unable to retrieve the PDF (status code {response.status_code})\"\n",
    "        \n",
    "        # Open the PDF file using pdfplumber\n",
    "        with pdfplumber.open(BytesIO(response.content)) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    full_text += text\n",
    "        \n",
    "        # Limit the text length\n",
    "        cleaned_text = full_text[:30000]\n",
    "        return cleaned_text\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"Error: Request timed out after 20 seconds\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs to fetch: 1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching URLs:   4%|▍         | 59/1388 [00:06<01:58, 11.25it/s]/tmp/ipykernel_23365/2310714524.py:166: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.text, 'lxml')\n",
      "Fetching URLs: 100%|██████████| 1388/1388 [44:33<00:00,  1.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated complete cache saved to /fs/archive/share/u2023000153/Search-o1/cache/url_cache_with_links_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 定义输入和输出路径\n",
    "    url_cache_path = \"/fs/archive/share/u2023000153/Search-o1/cache/url_cache_with_links.json\"\n",
    "    url_cache_new1_path = \"/fs/archive/share/u2023000153/Search-o1/cache/url_cache_with_links_1.json\"\n",
    "    \n",
    "    # 读取url_cache文件\n",
    "    with open(url_cache_path, 'r', encoding='utf-8') as f:\n",
    "        url_cache = json.load(f)\n",
    "    \n",
    "    # 提取所有错误的URL\n",
    "    urls = []\n",
    "    error_indicators = [\n",
    "        'limit exceeded',\n",
    "        'Error fetching URL',\n",
    "        'Account balance not enough to run this query, please recharge.',\n",
    "        'Invalid bearer token',\n",
    "        'HTTP error occurred', \n",
    "        'Error: Connection error occurred',\n",
    "        'Error: Request timed out',\n",
    "        'Unexpected error',\n",
    "        'Please turn on Javascript'\n",
    "    ]\n",
    "    \n",
    "    for url, page in url_cache.items():\n",
    "        if isinstance(page, str) and (any(indicator.lower() in page.lower() for indicator in error_indicators) or page.startswith('http') or len(page) < 10):\n",
    "            urls.append(url)\n",
    "            \n",
    "    print(f\"Total URLs to fetch: {len(urls)}\")\n",
    "    \n",
    "    # 重新获取错误URL的内容\n",
    "    if urls:\n",
    "        new_cache = fetch_page_content(urls, use_jina=False, show_progress=True, keep_links=True)\n",
    "        \n",
    "        # 将新获取的内容合并到原始缓存中\n",
    "        for url, content in new_cache.items():\n",
    "            url_cache[url] = content\n",
    "        \n",
    "    # 保存完整的url_cache到新文件\n",
    "    with open(url_cache_new1_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(url_cache, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Updated complete cache saved to {url_cache_new1_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取url_cache_new1_path并计算平均page字符数\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(url_cache_new1_path, 'r', encoding='utf-8') as f:\n",
    "    url_cache = json.load(f)\n",
    "\n",
    "# 收集所有页面的字符长度\n",
    "page_lengths = []\n",
    "for page in url_cache.values():\n",
    "    if isinstance(page, str):\n",
    "        page_lengths.append(len(page))\n",
    "\n",
    "# 计算统计信息\n",
    "total_chars = sum(page_lengths)\n",
    "valid_pages = len(page_lengths)\n",
    "avg_chars = total_chars / valid_pages if valid_pages > 0 else 0\n",
    "\n",
    "# 绘制直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(page_lengths, bins=10, edgecolor='black') # Changed bins from 50 to 10\n",
    "plt.title('Distribution of Page Lengths')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 添加平均值线\n",
    "plt.axvline(avg_chars, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {avg_chars:.0f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average characters per page: {avg_chars:.2f}\")\n",
    "print(f\"Total valid pages: {valid_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetched contents:\n",
      "---\n",
      "URL: https://en.wikipedia.org/wiki/Unlambda\n",
      "[Jump to content](https://en.wikipedia.org/wiki/<#bodyContent>)\n",
      "[ ![](https://en.wikipedia.org/static/images/icons/wikipedia.png) ![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg) ![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg) ](https://en.wikipedia.org/wiki/</wiki/Main_Page>)\n",
      "[ Search ](https://en.wikipedia.org/wiki/</wiki/Special:Search> \"Search Wikipedia \\[alt-shift-f\\]\")\n",
      "Search\n",
      "[ ![Banner logo](https://upload.wikimedia.org/wikipedia/commons/f/f9/Wikimania_logo.svg) **Wikimania 2025 Program Proposals are now open!** Click here to Apply NowThis application is open until Monday 31st March, 2025 end of day ](https://en.wikipedia.org/wiki/</wikimania.wikimedia.org/wiki/Special:MyLanguage/2025:Program>)\n",
      "[ ](https://en.wikipedia.org/wiki/</wikimania.wikimedia.org/wiki/Special:MyLanguage/2025:Program>)[ [ Help with translations! ] ](https://en.wikipedia.org/wiki/</meta.wikimedia.org/w/index.php?title=Special:Translate&group=Centralnotice-tgroup-WM2025Program_banner&language=en>)\n",
      "[ ](https://en.wikipedia.org/wiki/<#> \"Hide\")\n",
      "# Unlambda\n",
      "10 languages\n",
      "  * [Eesti](https://en.wikipedia.org/wiki/<https:/et.wikipedia.org/wiki/Unlambda> \"Unlambda – Estonian\")\n",
      "  * [Español](https://en.wikipedia.org/wiki/<https:/es.wikipedia.org/wiki/Unlambda> \"Unlambda – Spanish\")\n",
      "  * [Euskara](https://en.wikipedia.org/wiki/<https:/eu.wikipedia.org/wiki/Unlambda> \"Unlambda – Basque\")\n",
      "  * [Français](https://en.wikipedia.org/wiki/<https:/fr.wikipedia.org/wiki/Unlambda> \"Unlambda – French\")\n",
      "  * [Magyar](https://en.wikipedia.org/wiki/<https:/hu.wikipedia.org/wiki/Unlambda> \"Unlambda – Hungarian\")\n",
      "  * [日本語](https://en.wikipedia.org/wiki/<https:/ja.wikipedia.org/wiki/Unlambda> \"Unlambda – Japanese\")\n",
      "  * [Polski](https://en.wikipedia.org/wiki/<https:/pl.wikipedia.org/wiki/Unlambda> \"Unlambda – Polish\")\n",
      "  * [Português](https://en.wikipedia.org/wiki/<https:/pt.wikipedia.org/wiki/Unlambda> \"Unlambda – Portuguese\")\n",
      "  * [Русский](https://en.wikipedia.org/wiki/<https:/ru.wikipedia.org/wiki/Unlambda> \"Unlambda – Russian\")\n",
      "  * [Српски / srpski](https://en.wikipedia.org/wiki/<https:/sr.wikipedia.org/wiki/%D0%A3%D0%BD%D0%BB%D0%B0%D0%BC%D0%B1%D0%B4%D0%B0> \"Унламбда – Serbian\")\n",
      "\n",
      "\n",
      "[Edit links](https://en.wikipedia.org/wiki/<https:/www.wikidata.org/wiki/Special:EntityPage/Q670180#sitelinks-wikipedia> \"Edit interlanguage links\")\n",
      "From Wikipedia, the free encyclopedia\n",
      "[![](https://upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png)](https://en.wikipedia.org/wiki/</wiki/File:Question_book-new.svg>)| This article **needs additional citations for[verification](https://en.wikipedia.org/wiki/</wiki/Wikipedia:Verifiability> \"Wikipedia:Verifiability\")**. Please help [improve this article](https://en.wikipedia.org/wiki/</wiki/Special:EditPage/Unlambda> \"Special:EditPage/Unlambda\") by [adding citations to reliable sources](https://en.wikipedia.org/wiki/</wiki/Help:Referencing_for_beginners> \"Help:Referencing for beginners\"). Unsourced material may be challenged and removed._Find sources:_ [\"Unlambda\"](https://en.wikipedia.org/wiki/<https:/www.google.com/search?as_eq=wikipedia&q=%22Unlambda%22>) – [news](https://en.wikipedia.org/wiki/<https:/www.google.com/search?tbm=nws&q=%22Unlambda%22+-wikipedia&tbs=ar:1>) **·** [newspapers](https://en.wikipedia.org/wiki/<https:/www.google.com/search?&q=%22Unlambda%22&tbs=bkt:s&tbm=bks>) **·** [books](https://en.wikipedia.org/wiki/<https:/www.google.com/search?tbs=bks:1&q=%22Unlambda%22+-wikipedia>) **·** [scholar](https://en.wikipedia.org/wiki/<https:/scholar.google.com/scholar?q=%22Unlambda%22>) **·** [JSTOR](https://en.wikipedia.org/wiki/<https:/www.jstor.org/action/doBasicSearch?Query=%22Unlambda%22&acc=on&wc=on>) _( August 2020)__([Learn how and when to remove this message](https://en.wikipedia.org/wiki/</wiki/Help:Maintenance_template_removal> \"Help:Maintenance template removal\"))_  \n",
      "---|---  \n",
      "Functional programming language\n",
      "Unlambda[Paradigm](https://en.wikipedia.org/wiki/</wiki/Programming_paradigm> \"Programming paradigm\")| Nearly [pure](https://en.wikipedia.org/wiki/</wiki/Purely_functional_programming> \"Purely functional programming\") [functional](https://en.wikipedia.org/wiki/</wiki/Functional_programming> \"Functional programming\")  \n",
      "---|---  \n",
      "[Designed by](https://en.wikipedia.org/wiki/</wiki/Software_design> \"Software design\")| David Madore  \n",
      "[Developer](https://en.wikipedia.org/wiki/</wiki/Software_developer> \"Software developer\")| David Madore  \n",
      "First appeared| 28 June 1999; 25 years ago (1999-06-28)  \n",
      "[Stable release](https://en.wikipedia.org/wiki/</wiki/Software_release_life_cycle> \"Software release life cycle\")| 2.0.0 / 20 December 1999; 25 years ago (1999-12-20)  \n",
      "[Typing discipline](https://en.wikipedia.org/wiki/</wiki/Type_system> \"Type system\")| [Untyped](https://en.wikipedia.org/wiki/</wiki/Type_system> \"Type system\")  \n",
      "Implementation language| [Scheme](https://en.wikipedia.org/wiki/</wiki/Scheme_\\(programming_language\\)> \"Scheme \\(programming language\\)\"), [C](https://en.wikipedia.org/wiki/</wiki/C_\\(programming_language\\)> \"C \\(programming language\\)\"), [Java](https://en.wikipedia.org/wiki/</wiki/Java_\\(programming_language\\)> \"Java \\(programming language\\)\")  \n",
      "[License](https://en.wikipedia.org/wiki/</wiki/Software_license> \"Software license\")| [GPL](https://en.wikipedia.org/wiki/</wiki/GNU_General_Public_License> \"GNU General Public License\") 2.0 or later  \n",
      "Website| [www.madore.org/~david/programs/unlambda](https://en.wikipedia.org/wiki/<http:/www.madore.org/~david/programs/unlambda>)  \n",
      "**Unlambda** is a minimal, \"nearly [pure](https://en.wikipedia.org/wiki/</wiki/Purely_functional_language> \"Purely functional language\")\"[[1]](https://en.wikipedia.org/wiki/<#cite_note-chu2006-1>) [functional programming language](https://en.wikipedia.org/wiki/</wiki/Functional_programming_language> \"Functional programming language\") invented by David Madore. It is based on [combinatory logic](https://en.wikipedia.org/wiki/</wiki/Combinatory_logic> \"Combinatory logic\"), an expression system without the [lambda operator](https://en.wikipedia.org/wiki/</wiki/Lambda_calculus> \"Lambda calculus\") or free variables. It relies mainly on two built-in functions (`s` and `k`) and an apply operator (written ```, the [backquote](https://en.wikipedia.org/wiki/</wiki/Backquote> \"Backquote\") character). These alone make it [Turing-complete](https://en.wikipedia.org/wiki/</wiki/Turing-complete> \"Turing-complete\"), but there are also some [input/output](https://en.wikipedia.org/wiki/</wiki/Input/output> \"Input/output\") (I/O) functions to enable interacting with the user, some shortcut functions, and a [lazy evaluation](https://en.wikipedia.org/wiki/</wiki/Lazy_evaluation> \"Lazy evaluation\") function. Variables are unsupported. \n",
      "Unlambda is [free and open-source software](https://en.wikipedia.org/wiki/</wiki/Free_and_open-source_software> \"Free and open-source software\") distributed under a [GNU General Public License](https://en.wikipedia.org/wiki/</wiki/GNU_General_Public_License> \"GNU General Public License\") (GPL) 2.0 or later.[_[clarification needed](https://en.wikipedia.org/wiki/</wiki/Wikipedia:Please_clarify> \"Wikipedia:Please clarify\")_]\n",
      "## Basic principles\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=1> \"Edit section: Basic principles\")]\n",
      "As an [esoteric programming language](https://en.wikipedia.org/wiki/</wiki/Esoteric_programming_language> \"Esoteric programming language\"), Unlambda is meant as a demonstration of very pure functional programming rather than for practical use. Its main feature is the lack of conventional operators and data types—the only kind of data in the program are one-parameter functions. Data can nevertheless be simulated with appropriate functions as in the [lambda calculus](https://en.wikipedia.org/wiki/</wiki/Lambda_calculus> \"Lambda calculus\"). Multi-parameter functions can be represented via the method of [currying](https://en.wikipedia.org/wiki/</wiki/Currying> \"Currying\"). \n",
      "Unlambda is based on the principle of [abstraction elimination](https://en.wikipedia.org/wiki/</wiki/Abstraction_elimination> \"Abstraction elimination\"), or the elimination of all saved variables, including functions. As a purely functional language, Unlambda's functions are [first-class objects](https://en.wikipedia.org/wiki/</wiki/First-class_object> \"First-class object\"), and are the _only_ such objects. \n",
      "Here is an implementation of a [hello world program](https://en.wikipedia.org/wiki/</wiki/Hello_world_program> \"Hello world program\") in Unlambda:[[1]](https://en.wikipedia.org/wiki/<#cite_note-chu2006-1>)\n",
      "```\n",
      "`r```````````.H.e.l.l.o. .w.o.r.l.di\n",
      "\n",
      "```\n",
      "\n",
      "## Original built-in functions\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=2> \"Edit section: Original built-in functions\")]\n",
      "The notation `._x_`denotes a function which takes one argument and returns it unchanged, printing the single character _x_ as a side effect when it is invoked. `i` represents the version of the identity function that has no such side effect; it is used here as a dummy argument. The program ``.di` applies the `d`-printing function to a dummy argument of `i`, returning `i` and printing the letter `d` as a side effect. Similarly, ```.l.di` first applies `.l` to `.d`, printing the letter `l` and returning `.d`; this result of `.d` is then applied to `i` as in the previous example. The function `r` is [syntactic sugar](https://en.wikipedia.org/wiki/</wiki/Syntactic_sugar> \"Syntactic sugar\") for the function that prints a newline character. \n",
      "Other important features provided by Unlambda include the `k` and `s` functions. `k` manufactures constant functions: the result of ``k_x_`is a function which, when invoked, returns _x_. Thus the value of ```k_xy_`is _x_ for any _x_ and _y_. \n",
      "`s` is a generalized evaluation operator. ````s_xyz_`evaluates to``` _xz_`_yz_`for any _x_ , _y_ , and _z_. It is a remarkable fact that `s` and `k` are sufficient to perform any calculation, as described in [SKI combinator calculus](https://en.wikipedia.org/wiki/</wiki/SKI_combinator_calculus> \"SKI combinator calculus\"). As a brief example, the identity function `i` can be implemented as ```skk`, since ````skk_x_`yields _x_ for all _x_. \n",
      "Unlambda's one flow control construct is [call with current continuation](https://en.wikipedia.org/wiki/</wiki/Call_with_current_continuation> \"Call with current continuation\"), denoted `c`. When an expression of the form ``c_x_`is evaluated, a special _continuation_ object is constructed, representing the state of the interpreter at that moment. Then _x_ is evaluated, and then the result is given the continuation object as an argument. If the continuation is never applied to an argument, the value of the ``c_x_`expression is the same as the value of _x_. But if the continuation object is applied to a value _y_ , execution of _x_ is immediately aborted, and the value of the entire ``c_x_`expression is _y_. \n",
      "Unlambda's execution semantics are normally [eager evaluation](https://en.wikipedia.org/wiki/</wiki/Eager_evaluation> \"Eager evaluation\"), but a [lazy evaluation](https://en.wikipedia.org/wiki/</wiki/Lazy_evaluation> \"Lazy evaluation\") option exists, indicated by the use of the `d` operator. Usually, to evaluate an expression of the form ``_xy_`, unlambda first evaluates _x_ , then _y_ , and then applies _x_ to _y_. However, if _x_ evaluates to the special value `d`, then _y_ is _not_ evaluated; instead, the value of the expression ``d_y_`is a special \"delayed computation\" object, which, when applied to an argument _z_ , evaluates _y_ , and then applies its value to _z_. In the absence of side effects, this is exactly the same as ``i_y_`. The difference is that``i _y_`executes any side effects in _y_ immediately, whereas ``d_y_`defers the side effects until the result is applied to another argument.\n",
      "Unlambda's next built-in operator is `v`, which ignores its argument and returns `v`. This feature is not strictly necessary, since `v` could be implemented as ```s`k``s``s`kskk`k``s``s`kskk`, but it is supplied as a convenience. (This expression above is simply ``Yk`, where `Y` denotes a [fixed point combinator](https://en.wikipedia.org/wiki/</wiki/Fixed_point_combinator> \"Fixed point combinator\").) \n",
      "## Version 2 built-in functions\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=3> \"Edit section: Version 2 built-in functions\")]\n",
      "More built-ins were introduced in Unlambda version 2. [Input](https://en.wikipedia.org/wiki/</wiki/Input/output> \"Input/output\") is facilitated by operators `@` and `?_u_`. When`@` is applied to a function _x_ , a character is read from input, and stored as the \"current character\"; then _x_ is applied to `i`. However, if no more characters were available on input, the _current character_ is left undefined, and _x_ is applied to `v` instead. When a function `?_u_`is applied to a function _x_ , the result is the evaluation of ``_x_i`if the current character is _u_ , otherwise ``_x_v`is evaluated.\n",
      "There is also a \"reprint\" operator `|`. When ``|_x_`is evaluated, the function _x_ is applied to `._u_`if _u_ is the current character, or to `v` if there is no current character. \n",
      "Finally, there is an exit operator `e`. When `e` is applied to _x_ , the execution of the program is terminated, and _x_ is taken as the result of the program (most of the currently existing interpreters ignore the result anyway). \n",
      "## See also\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=4> \"Edit section: See also\")]\n",
      "  * [Iota and Jot](https://en.wikipedia.org/wiki/</wiki/Iota_and_Jot> \"Iota and Jot\")\n",
      "  * [SKI combinator calculus](https://en.wikipedia.org/wiki/</wiki/SKI_combinator_calculus> \"SKI combinator calculus\")\n",
      "\n",
      "\n",
      "## References\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=5> \"Edit section: References\")]\n",
      "  1. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/<#cite_ref-chu2006_1-0>) [_**b**_](https://en.wikipedia.org/wiki/<#cite_ref-chu2006_1-1>) Chu-Carroll, Mark C. (2006-08-11). [\"Friday Pathological Programming: Unlambda, or Programming Without Variables\"](https://en.wikipedia.org/wiki/<http:/scienceblogs.com/goodmath/2006/08/11/friday-pathological-programmin-3/>). _Good Math, Bad Math (blog)_. ScienceBlogs.\n",
      "\n",
      "\n",
      "  * Felix-Hernandez Campos (1 April 2002), _[Lecture 28: More on functional programming](https://en.wikipedia.org/wiki/<http:/www.cs.unc.edu/~stotts/144/lectures/lect28apr.pdf>)_ , University of North Carolina COMP144\n",
      "  * 原 悠 (Yutaka Hara) (2008). [_Rubyで作る奇妙なプログラミング言語_](https://en.wikipedia.org/wiki/<https:/books.google.com/books?id=LlR6__OpAxoC&dq=%22Unlambda%22&pg=PA205>) (in Japanese). Tōkyō: Mainichikomyunikēshonzu. pp. 205–214. [ISBN](https://en.wikipedia.org/wiki/</wiki/ISBN_\\(identifier\\)> \"ISBN \\(identifier\\)\") [978-4-8399-2784-4](https://en.wikipedia.org/wiki/</wiki/Special:BookSources/978-4-8399-2784-4> \"Special:BookSources/978-4-8399-2784-4\").\n",
      "\n",
      "\n",
      "## External links\n",
      "[[edit](https://en.wikipedia.org/wiki/</w/index.php?title=Unlambda&action=edit&section=6> \"Edit section: External links\")]\n",
      "  * [Official website](https://en.wikipedia.org/wiki/<http:/www.madore.org/~david/programs/unlambda>)\n",
      "\n",
      "\n",
      "hide\n",
      "  * [v](https://en.wikipedia.org/wiki/</wiki/Template:Esoteric_programming_languages> \"Template:Esoteric programming languages\")\n",
      "  * [t](https://en.wikipedia.org/wiki/</wiki/Template_talk:Esoteric_programming_languages> \"Template talk:Esoteric programming languages\")\n",
      "  * [e](https://en.wikipedia.org/wiki/</wiki/Special:EditPage/Template:Esoteric_programming_languages> \"Special:EditPage/Template:Esoteric programming languages\")\n",
      "\n",
      "[Esoteric programming languages](https://en.wikipedia.org/wiki/</wiki/Esoteric_programming_language> \"Esoteric programming language\")  \n",
      "---  \n",
      "  * [Beatnik](https://en.wikipedia.org/wiki/</wiki/Beatnik_\\(programming_language\\)> \"Beatnik \\(programming language\\)\")\n",
      "  * [Befunge](https://en.wikipedia.org/wiki/</wiki/Befunge> \"Befunge\")\n",
      "  * [Brainfuck](https://en.wikipedia.org/wiki/</wiki/Brainfuck> \"Brainfuck\")\n",
      "  * [FRACTRAN](https://en.wikipedia.org/wiki/</wiki/FRACTRAN> \"FRACTRAN\")\n",
      "  * [INTERCAL](https://en.wikipedia.org/wiki/</wiki/INTERCAL> \"INTERCAL\")\n",
      "  * [Iota and Jot](https://en.wikipedia.org/wiki/</wiki/Iota_and_Jot> \"Iota and Jot\")\n",
      "  * [JSFuck](https://en.wikipedia.org/wiki/</wiki/JSFuck> \"JSFuck\")\n",
      "  * [Leet](https://en.wikipedia.org/wiki/</wiki/Leet_\\(programming_language\\)> \"Leet \\(programming language\\)\")\n",
      "  * [LOLCODE](https://en.wikipedia.org/wiki/</wiki/LOLCODE> \"LOLCODE\")\n",
      "  * [Malbolge](https://en.wikipedia.org/wiki/</wiki/Malbolge> \"Malbolge\")\n",
      "  * [One-instruction set computer](https://en.wikipedia.org/wiki/</wiki/One-instruction_set_computer> \"One-instruction set computer\")\n",
      "  * [Piet](https://en.wikipedia.org/wiki/</wiki/Piet_\\(programming_language\\)> \"Piet \\(programming language\\)\")\n",
      "  * [Shakespeare Programming Language](https://en.wikipedia.org/wiki/</wiki/Shakespeare_Programming_Language> \"Shakespeare Programming Language\")\n",
      "  * Unlambda\n",
      "  * [Whitespace](https://en.wikipedia.org/wiki/</wiki/Whitespace_\\(programming_language\\)> \"Whitespace \\(programming language\\)\")\n",
      "\n",
      "  \n",
      "![](https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png) [Category](https://en.wikipedia.org/wiki/</wiki/Category:Esoteric_programming_languages> \"Category:Esoteric programming languages\")  \n",
      "Retrieved from \"[https://en.wikipedia.org/w/index.php?title=Unlambda&oldid=1239721564](https://en.wikipedia.org/wiki/<https:/en.wikipedia.org/w/index.php?title=Unlambda&oldid=1239721564>)\"\n",
      "[Categories](https://en.wikipedia.org/wiki/</wiki/Help:Category> \"Help:Category\"): \n",
      "  * [Esoteric programming languages](https://en.wikipedia.org/wiki/</wiki/Category:Esoteric_programming_languages> \"Category:Esoteric programming languages\")\n",
      "  * [Functional languages](https://en.wikipedia.org/wiki/</wiki/Category:Functional_languages> \"Category:Functional languages\")\n",
      "\n",
      "\n",
      "Hidden categories: \n",
      "  * [Articles needing additional references from August 2020](https://en.wikipedia.org/wiki/</wiki/Category:Articles_needing_additional_references_from_August_2020> \"Category:Articles needing additional references from August 2020\")\n",
      "  * [All articles needing additional references](https://en.wikipedia.org/wiki/</wiki/Category:All_articles_needing_additional_references> \"Category:All articles needing additional references\")\n",
      "  * [Articles with short description](https://en.wikipedia.org/wiki/</wiki/Category:Articles_with_short_description> \"Category:Articles with short description\")\n",
      "  * [Short description matches Wikidata](https://en.wikipedia.org/wiki/</wiki/Category:Short_description_matches_Wikidata> \"Category:Short description matches Wikidata\")\n",
      "  * [Wikipedia articles needing clarification from March 2022](https://en.wikipedia.org/wiki/</wiki/Category:Wikipedia_articles_needing_clarification_from_March_2022> \"Category:Wikipedia articles needing clarification from March 2022\")\n",
      "  * [CS1 Japanese-language sources (ja)](https://en.wikipedia.org/wiki/</wiki/Category:CS1_Japanese-language_sources_\\(ja\\)> \"Category:CS1 Japanese-language sources \\(ja\\)\")\n",
      "  * [Official website different in Wikidata and Wikipedia](https://en.wikipedia.org/wiki/</wiki/Category:Official_website_different_in_Wikidata_and_Wikipedia> \"Category:Official website different in Wikidata and Wikipedia\")\n",
      "\n",
      "\n",
      "Search\n",
      "Search\n",
      "Unlambda\n",
      "[ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>) [ ](https://en.wikipedia.org/wiki/<#>)\n",
      "10 languages [ Add topic ](https://en.wikipedia.org/wiki/<#>)\n",
      "[](https://en.wikipedia.org/wiki/<https:/en.wikipedia.org/wiki/Unlambda?action=edit>)\n",
      "  *[v]: View this template\n",
      "  *[t]: Discuss this template\n",
      "  *[e]: Edit this template\n",
      "...\n",
      "---\n",
      "URL: https://www.researchgate.net/scientific-contributions/Iram-Khan-2100479969\n",
      "# Krzysztof Azierski’s scientific contributions\n",
      "## What is this page?\n",
      "This page lists works of an author who doesn't have a ResearchGate profile or hasn't added the works to their profile yet. It is automatically generated from public (personal) data to further our legitimate goal of comprehensive and accurate scientific recordkeeping. If you are this author and want this page removed, please [let us know](https://www.researchgate.net/scientific-contributions/<https:/help.researchgate.net/hc/en-us/requests/new?ticket_form_id=13146494812305>).\n",
      "## Publications (1)\n",
      "[Fatty alcohols-stabilized foams produced of aqueous solutions of cetylpyridinium chloride](https://www.researchgate.net/scientific-contributions/<https:/www.researchgate.net/publication/295569558_Fatty_alcohols-stabilized_foams_produced_of_aqueous_solutions_of_cetylpyridinium_chloride?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19>)\n",
      "  * [Article](https://www.researchgate.net/scientific-contributions/<publication/295569558_Fatty_alcohols-stabilized_foams_produced_of_aqueous_solutions_of_cetylpyridinium_chloride>)\n",
      "\n",
      "\n",
      "June 2014\n",
      "·\n",
      "11 Reads\n",
      "PRZEMYSŁ CHEMICZNY\n",
      "[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Krzysztof Azierski](https://www.researchgate.net/scientific-contributions/<scientific-contributions/Krzysztof-Azierski-2100479969>)\n",
      "·\n",
      "[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Jakub Jakubiec](https://www.researchgate.net/scientific-contributions/<scientific-contributions/Jakub-Jakubiec-2100578482>)\n",
      "·\n",
      "[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Andrzej Mizerski](https://www.researchgate.net/scientific-contributions/<scientific-contributions/Andrzej-Mizerski-2100459062>)\n",
      "Aq. soln. of cetylpyridinium chloride (6 g/L) was dild. with BuOCH2CH2OH, EtOCH2CH2OEt or (BuOCH2CH)(2)O, stabilized by addn. of C11H23OH, C12H25OH, C16H33OH or a corn. mixt. of C12H25OH and C14H29OH and studied for foaming ability and foam stability at room temp. The highest expansion ratio (11.1) and foam half-life time (20.2 mm) was achieved when C11H23OH was used as solvent and EtOCH2CH2OEt used as stabilizer.\n",
      "Read more\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Define URLs to fetch content from\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Unlambda\",\n",
    "    \"https://www.researchgate.net/scientific-contributions/Iram-Khan-2100479969\",\n",
    "    # \"https://onlinelibrary.wiley.com/doi/full/10.1111/jph.12451\", \n",
    "    # \"https://journals.le.ac.uk/ojs1/index.php/jist/article/view/733\",\n",
    "    # \"https://oda.oslomet.no/oda-xmlui/handle/10642/3162\",\n",
    "    # \"https://www.journalijar.com/article/26843/a-simple-model-for-analyzing-the-customer-retention-comparing-rural-and-urban-store\",\n",
    "    # \"https://www.mdpi.com/2076-2607/11/1/123?type=check_update&version=2\",\n",
    "    # \"https://arxiv.org/\",\n",
    "    # \"https://www.virtuerestaurant.com\",\n",
    "    # \"https://replit.com\",\n",
    "    # \"https://github.com/sunnynexus/Search-o1\",\n",
    "    # \"https://www.base-search.net\",\n",
    "    # \"https://www.virtuerestaurant.com/menus\",\n",
    "]\n",
    "\n",
    "# Fetch content from URLs with progress bar\n",
    "# contents = fetch_page_content(urls, keep_links=True)\n",
    "contents = fetch_page_content(urls, use_web_parser=True)\n",
    "\n",
    "# Print URL and content for each result\n",
    "print(\"\\nFetched contents:\")\n",
    "for url, content in contents.items():\n",
    "    print(f\"---\\nURL: {url}\")\n",
    "    print(f\"{content[:30000]}...\")  # Print first 500 chars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
